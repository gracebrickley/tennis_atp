{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"review_and_category_analytics\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAMES: Katherine Brickley (kgb3mf), Jack Peele, Will McDevitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('/project/ds5559/group14_tennis/tennis_atp/atp_matches_2012.csv', header = True)\n",
    "data_path = '/project/ds5559/group14_tennis/tennis_atp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# assign path\n",
    "path, dirs, files = next(os.walk(data_path))\n",
    "file_count = len(files)\n",
    "# create empty list\n",
    "dataframes_list = []\n",
    " \n",
    "# append datasets to the list\n",
    "for i in range(file_count):\n",
    "    if '.csv' in files[i]:\n",
    "        temp_df =  spark.read.csv(data_path+files[i])\n",
    "        dataframes_list.append(temp_df)\n",
    "     \n",
    "# display datasets\n",
    "'''for dataset in dataframes_list:\n",
    "    dataset.show(5)\n",
    "    print('\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-------+---------+-------------+------------+---------+---------+-----------+------------+-------------------+-----------+---------+----------+-------------+--------+----------+-----------+--------------------+----------+--------+---------+-------------+-----------+-------+-----+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+\n",
      "|                 _c0|              _c1|    _c2|      _c3|          _c4|         _c5|      _c6|      _c7|        _c8|         _c9|               _c10|       _c11|     _c12|      _c13|         _c14|    _c15|      _c16|       _c17|                _c18|      _c19|    _c20|     _c21|         _c22|       _c23|   _c24| _c25|   _c26| _c27|_c28|  _c29|   _c30|    _c31|    _c32|   _c33|     _c34|     _c35| _c36|_c37|  _c38|   _c39|    _c40|    _c41|   _c42|     _c43|     _c44|       _c45|              _c46|      _c47|             _c48|\n",
      "+--------------------+-----------------+-------+---------+-------------+------------+---------+---------+-----------+------------+-------------------+-----------+---------+----------+-------------+--------+----------+-----------+--------------------+----------+--------+---------+-------------+-----------+-------+-----+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+\n",
      "|          tourney_id|     tourney_name|surface|draw_size|tourney_level|tourney_date|match_num|winner_id|winner_seed|winner_entry|        winner_name|winner_hand|winner_ht|winner_ioc|   winner_age|loser_id|loser_seed|loser_entry|          loser_name|loser_hand|loser_ht|loser_ioc|    loser_age|      score|best_of|round|minutes|w_ace|w_df|w_svpt|w_1stIn|w_1stWon|w_2ndWon|w_SvGms|w_bpSaved|w_bpFaced|l_ace|l_df|l_svpt|l_1stIn|l_1stWon|l_2ndWon|l_SvGms|l_bpSaved|l_bpFaced|winner_rank|winner_rank_points|loser_rank|loser_rank_points|\n",
      "|1993-M-SA-ITA-01A...|Italy 1 Masters 1| Carpet|       32|            S|    19930102|        1|   101280|          1|        null|    Mikael Stadling|          L|      188|       SWE|26.3846680356|  102437|      null|       null|       Adrian Voinea|         R|     185|      ROU|18.3956194387|7-6 5-7 7-5|      3|  R32|   null| null|null|  null|   null|    null|    null|   null|     null|     null| null|null|  null|   null|    null|    null|   null|     null|     null|        360|              null|       332|             null|\n",
      "|1993-M-SA-ITA-01A...|Italy 1 Masters 1| Carpet|       32|            S|    19930102|        2|   101405|       null|        null|      Ugo Colombini|          R|      183|       ITA|25.2265571526|  101871|      null|       null|   Alexander Reichel|         R|     193|      USA|21.8234086242|4-6 6-4 7-6|      3|  R32|   null| null|null|  null|   null|    null|    null|   null|     null|     null| null|null|  null|   null|    null|    null|   null|     null|     null|        947|              null|       591|             null|\n",
      "|1993-M-SA-ITA-01A...|Italy 1 Masters 1| Carpet|       32|            S|    19930102|        3|   102491|       null|           Q|     Alex Radulescu|          R|      185|       GER|18.0588637919|  102008|      null|          Q|         Johan Alven|         R|    null|      SWE|20.9582477755|6-3 2-6 6-4|      3|  R32|   null| null|null|  null|   null|    null|    null|   null|     null|     null| null|null|  null|   null|    null|    null|   null|     null|     null|        464|              null|       708|             null|\n",
      "|1993-M-SA-ITA-01A...|Italy 1 Masters 1| Carpet|       32|            S|    19930102|        4|   101947|       null|          WC|Vincenzo Santopadre|          L|      183|       ITA|21.3826146475|  102050|         8|       null|Marco Meneschincheri|         L|     180|      ITA|20.6762491444|    6-2 6-0|      3|  R32|   null| null|null|  null|   null|    null|    null|   null|     null|     null| null|null|  null|   null|    null|    null|   null|     null|     null|        422|              null|       416|             null|\n",
      "+--------------------+-----------------+-------+---------+-------------+------------+---------+---------+-----------+------------+-------------------+-----------+---------+----------+-------------+--------+----------+-----------+--------------------+----------+--------+---------+-------------+-----------+-------+-----+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes_list[5].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/qumulo/qhome/kgb3mf/repos\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset = dataframes_list[0]\n",
    "first_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHange this to full dataset later\n",
    "print(\"Number of columns:\", len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column called numNull then filter out if over 6\n",
    "data = data.withColumn(\"numNull\", sum(data[col].isNull().cast('int') for col in data.columns))\n",
    "\n",
    "data = data.filter(data.numNull < 6)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "winners = [col for col in data.columns if col.startswith('w_')]\n",
    "winners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losers = [col for col in data.columns if col.startswith('l_')]\n",
    "losers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = data.select(winners)\n",
    "new_columns = [col.strip('w_') for col in winners]\n",
    "\n",
    "for i in range(0, len(final_df.columns)):\n",
    "    final_df = final_df.withColumnRenamed(final_df.columns[i], new_columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "final_df = final_df.withColumn('result', lit(1))\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2 = data.select(losers)\n",
    "\n",
    "for i in range(0, len(final_df2.columns)):\n",
    "    final_df2 = final_df2.withColumnRenamed(final_df2.columns[i], new_columns[i])\n",
    "    \n",
    "final_df2 = final_df2.withColumn('result', lit(0))\n",
    "final_df2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_df.union(final_df2)\n",
    "\n",
    "final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change names of some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumnRenamed('1stWon', 'firstWon')\n",
    "final = final.withColumnRenamed('2ndWon', 'secondWon')\n",
    "final = final.withColumnRenamed('1stIn', 'firstIn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  5352\n"
     ]
    }
   ],
   "source": [
    "print('Number of records: ', final.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  10\n"
     ]
    }
   ],
   "source": [
    "print('Number of columns: ', len(final.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical summary of response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2676\n",
      "0    2676\n",
      "Name: result, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            result\n",
       "count  5352.000000\n",
       "mean      0.500000\n",
       "std       0.500047\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.500000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final.toPandas().result.value_counts())\n",
    "\n",
    "final.toPandas().describe()\n",
    "\n",
    "# This is a binary response variable, so the only thing that really matters is value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.groupBy('result').count().orderBy('result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical summary of potential predictor variables (if there are a large number of predictors, select the top 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            result\n",
       "count  5352.000000\n",
       "mean      0.500000\n",
       "std       0.500047\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.500000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ace</th>\n",
       "      <th>df</th>\n",
       "      <th>svpt</th>\n",
       "      <th>firstIn</th>\n",
       "      <th>firstWon</th>\n",
       "      <th>secondWon</th>\n",
       "      <th>SvGms</th>\n",
       "      <th>bpSaved</th>\n",
       "      <th>bpFaced</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352</td>\n",
       "      <td>5352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>196</td>\n",
       "      <td>136</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>583</td>\n",
       "      <td>1089</td>\n",
       "      <td>102</td>\n",
       "      <td>144</td>\n",
       "      <td>214</td>\n",
       "      <td>358</td>\n",
       "      <td>782</td>\n",
       "      <td>773</td>\n",
       "      <td>505</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ace    df  svpt firstIn firstWon secondWon SvGms bpSaved bpFaced  \\\n",
       "count   5352  5352  5352    5352     5352      5352  5352    5352    5352   \n",
       "unique    38    17   196     136      100        54    36      21      29   \n",
       "top        3     2    54      35       29        12    10       3       6   \n",
       "freq     583  1089   102     144      214       358   782     773     505   \n",
       "mean     NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "std      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "min      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "25%      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "50%      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "75%      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "max      NaN   NaN   NaN     NaN      NaN       NaN   NaN     NaN     NaN   \n",
       "\n",
       "             result  \n",
       "count   5352.000000  \n",
       "unique          NaN  \n",
       "top             NaN  \n",
       "freq            NaN  \n",
       "mean       0.500000  \n",
       "std        0.500047  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.500000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.toPandas().describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn(\"ace\",final.ace.cast('double'))\n",
    "final = final.withColumn(\"df\",final.df.cast('double'))\n",
    "final = final.withColumn(\"svpt\",final.svpt.cast('double'))\n",
    "final = final.withColumn(\"firstIn\",final.firstIn.cast('double'))\n",
    "final = final.withColumn(\"firstWon\",final.firstWon.cast('double'))\n",
    "final = final.withColumn(\"secondWon\",final.secondWon.cast('double'))\n",
    "final = final.withColumn(\"SvGms\",final.SvGms.cast('double'))\n",
    "final = final.withColumn(\"bpSaved\",final.bpSaved.cast('double'))\n",
    "final = final.withColumn(\"bpFaced\",final.bpFaced.cast('double'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 314\n",
    "train_test = [0.8, 0.2]\n",
    "train_data, test_data = final.randomSplit(train_test, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+\n",
      "|ace| df|svpt|firstIn|firstWon|secondWon|SvGms|bpSaved|bpFaced|result|            features|\n",
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+\n",
      "|0.0|0.0| 8.0|    5.0|     5.0|      3.0|  2.0|    0.0|    0.0|     1|[0.0,0.0,8.0,5.0,...|\n",
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['ace','df','svpt','firstIn','firstWon','secondWon','SvGms','bpSaved','bpFaced'],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "transformed = assembler.transform(train_data)\n",
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "|ace| df|svpt|firstIn|firstWon|secondWon|SvGms|bpSaved|bpFaced|result|            features|              scaled|\n",
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "|0.0|0.0| 8.0|    5.0|     5.0|      3.0|  2.0|    0.0|    0.0|     1|[0.0,0.0,8.0,5.0,...|[0.0,0.0,0.268092...|\n",
      "|0.0|0.0| 8.0|    6.0|     6.0|      2.0|  2.0|    0.0|    0.0|     1|[0.0,0.0,8.0,6.0,...|[0.0,0.0,0.268092...|\n",
      "|0.0|0.0|10.0|    5.0|     5.0|      3.0|  2.0|    0.0|    0.0|     1|[0.0,0.0,10.0,5.0...|[0.0,0.0,0.335116...|\n",
      "|0.0|0.0|31.0|   22.0|    17.0|      5.0|  5.0|    3.0|    3.0|     1|[0.0,0.0,31.0,22....|[0.0,0.0,1.038860...|\n",
      "|0.0|0.0|40.0|   28.0|    22.0|      8.0|  7.0|    0.0|    0.0|     1|[0.0,0.0,40.0,28....|[0.0,0.0,1.340464...|\n",
      "|0.0|0.0|46.0|   38.0|    27.0|      5.0|  8.0|    0.0|    1.0|     1|[0.0,0.0,46.0,38....|[0.0,0.0,1.541534...|\n",
      "|0.0|0.0|52.0|   34.0|    27.0|     13.0|  9.0|    4.0|    4.0|     1|[0.0,0.0,52.0,34....|[0.0,0.0,1.742604...|\n",
      "|0.0|0.0|54.0|   22.0|    16.0|     19.0|  9.0|    1.0|    3.0|     1|[0.0,0.0,54.0,22....|[0.0,0.0,1.809627...|\n",
      "|0.0|0.0|55.0|   34.0|    28.0|     14.0| 10.0|    1.0|    1.0|     1|[0.0,0.0,55.0,34....|[0.0,0.0,1.843139...|\n",
      "|0.0|0.0|58.0|   43.0|    29.0|      6.0| 10.0|    2.0|    4.0|     1|[0.0,0.0,58.0,43....|[0.0,0.0,1.943673...|\n",
      "+---+---+----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol = \"scaled\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "\n",
    "\n",
    "df2 = scalerModel.transform(transformed)\n",
    "#df2.groupBy('result').count().show()\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_cols = [x for x in df2.columns if x != 'result']\n",
    "\n",
    "# cols = [x for x in df2.columns if x == 'result' or x == 'scaled']\n",
    "# new_df = df2[cols]\n",
    "# new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol='result',\n",
    "                        featuresCol='scaled',\n",
    "                        maxIter=100, \n",
    "                        regParam=0.8, \n",
    "                        elasticNetParam=0)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,[],[])\n"
     ]
    }
   ],
   "source": [
    "print(lrModel.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|probability|prediction|\n",
      "+-----------+----------+\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "|[0.5,0.5]  |0.0       |\n",
      "+-----------+----------+\n",
      "only showing top 50 rows\n",
      "\n",
      "Area under PR Curve: 0.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(df2)\n",
    "lrPred.select('probability','prediction').show(50,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"result\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under PR Curve:\", aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrModel.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "| ace| df| svpt|firstIn|firstWon|secondWon|SvGms|bpSaved|bpFaced|result|            features|              scaled|\n",
      "+----+---+-----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "|10.0|0.0| 92.0|   57.0|    41.0|     17.0| 15.0|    4.0|    7.0|     1|[10.0,0.0,92.0,57...|[1.92735559605842...|\n",
      "|23.0|1.0|104.0|   55.0|    47.0|     31.0| 17.0|    3.0|    3.0|     1|[23.0,1.0,104.0,5...|[4.43291787093437...|\n",
      "| 8.0|3.0| 71.0|   31.0|    26.0|     26.0| 11.0|    5.0|    6.0|     1|[8.0,3.0,71.0,31....|[1.54188447684674...|\n",
      "| 1.0|2.0| 75.0|   49.0|    31.0|     16.0| 13.0|    3.0|    6.0|     1|[1.0,2.0,75.0,49....|[0.19273555960584...|\n",
      "|10.0|1.0| 56.0|   28.0|    23.0|     18.0|  8.0|    2.0|    2.0|     1|[10.0,1.0,56.0,28...|[1.92735559605842...|\n",
      "| 4.0|2.0| 66.0|   40.0|    32.0|     14.0| 11.0|    6.0|    7.0|     1|[4.0,2.0,66.0,40....|[0.77094223842337...|\n",
      "| 6.0|0.0| 52.0|   30.0|    24.0|     15.0|  9.0|    2.0|    2.0|     1|[6.0,0.0,52.0,30....|[1.15641335763505...|\n",
      "| 7.0|2.0| 92.0|   53.0|    36.0|     23.0| 15.0|    4.0|    7.0|     1|[7.0,2.0,92.0,53....|[1.34914891724089...|\n",
      "| 8.0|4.0| 91.0|   51.0|    38.0|     22.0| 14.0|    3.0|    4.0|     1|[8.0,4.0,91.0,51....|[1.54188447684674...|\n",
      "| 3.0|0.0| 71.0|   53.0|    32.0|     10.0| 10.0|    6.0|    9.0|     1|[3.0,0.0,71.0,53....|[0.57820667881752...|\n",
      "+----+---+-----+-------+--------+---------+-----+-------+-------+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
